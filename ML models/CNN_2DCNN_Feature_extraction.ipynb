{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f66aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path\n",
    "import os\n",
    "import random\n",
    "from os.path import join\n",
    "# from pathlib2 import Path\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras import backend as K, callbacks\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbb21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CUTOFF = '2016-04-21' # limit date for training\n",
    "TRAIN_VALID_RATIO = 0.75 # Size of training set\n",
    "\n",
    "# Data Generator\n",
    "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
    "    \"\"\"As a generator to produce samples for Keras model\"\"\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys())) # One filename from dataset\n",
    "        df = data[key]\n",
    "        # extract column name for features except target column and save it as an array\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # assign index for train set / Dataframe\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        # Set point of split for train and validation set / integer\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        if kind == 'train':\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   # range for the validation set\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)      # pick one time step / integer\n",
    "            n = (df.index == t).argmax()  # find its position in the dataframe / argmax return index where the condition is true\n",
    "            # Check if n length is greater than 0\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # can't get enough data for one sequence length\n",
    "            # Get the dataframe \n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            #              # ARRAY                      # 1 / integer\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            # Create 1 dimensional array\n",
    "            X, y = zip(*batch) # The * in a function call \"unpacks\" a list (or other iterable), making each of its elements a separate argument.\n",
    "            # Create 4 dimensional array for X, and 1 for y\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            # returns a generator object\n",
    "            yield X, y\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b349a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns:  dict_keys(['NYA', 'S&P', 'NASDAQ', 'RUT', 'DJI'])\n",
      "Data Lenght:  5\n"
     ]
    }
   ],
   "source": [
    "# Dictionary consist of dataframe for each index\n",
    "data = {}\n",
    "DATADIR = '../datasets/processed'\n",
    "for filename in os.listdir(DATADIR):\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue # read only the CSV files\n",
    "    filepath = os.path.join(DATADIR, filename)\n",
    "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "    name = X[\"Name\"][0] # All name has the same value which is the stock company name\n",
    "    del X[\"Name\"]\n",
    "    cols = X.columns\n",
    "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int) # Will input 1 or 0 for up and down\n",
    "    X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    index = X.index[X.index > TRAIN_TEST_CUTOFF] # Get index until TRAIN_TEST_CUTOFF\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)] # Get train set \n",
    "    # To set all data to standard value. Formula: ((x1-mean)/standardDeviation)\n",
    "    scaler = StandardScaler().fit(X.loc[index, cols]) # Standardize train set\n",
    "    # Save scale transformed dataframe\n",
    "    X[cols] = scaler.transform(X[cols])\n",
    "    # save each transformed dataframe on data dictionary\n",
    "    data[name] = X\n",
    "\n",
    "print(\"Data columns: \", data.keys())\n",
    "print(\"Data Lenght: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9df6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "# CNN structure\n",
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2999ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall = (TP/(TP+FN))\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Precision = (TP/(TP+FP))\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# F1 Score metrics\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# Average of the F1 from positive and negative classification (F1-macro metric)\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc8762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Checkpoints to interupt or resume model training\n",
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
    "callbacks = [\n",
    "    callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\", verbose=0,\n",
    "                    save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef9d8a",
   "metadata": {},
   "source": [
    "We set up a filename template checkpoint_path and ask Keras to fill in the epoch number as well as validation F1 score into the filename. We save it by monitoring the validationâ€™s F1 metric, and this metric is supposed to increase when the model gets better. Hence we pass in the mode=\"max\" to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a6109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 158s 389ms/step - loss: 0.4443 - acc: 0.5563 - f1macro: 0.3572 - val_loss: 0.5023 - val_acc: 0.4977 - val_f1macro: 0.3317\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 156s 392ms/step - loss: 0.4431 - acc: 0.5569 - f1macro: 0.3572 - val_loss: 0.4727 - val_acc: 0.5273 - val_f1macro: 0.3447\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 166s 415ms/step - loss: 0.4398 - acc: 0.5602 - f1macro: 0.3585 - val_loss: 0.4649 - val_acc: 0.5352 - val_f1macro: 0.3481\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 160s 401ms/step - loss: 0.4423 - acc: 0.5577 - f1macro: 0.3575 - val_loss: 0.5039 - val_acc: 0.4961 - val_f1macro: 0.3311\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 155s 389ms/step - loss: 0.4420 - acc: 0.5580 - f1macro: 0.3577 - val_loss: 0.4735 - val_acc: 0.5266 - val_f1macro: 0.3443\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 152s 380ms/step - loss: 0.4183 - acc: 0.5847 - f1macro: 0.4331 - val_loss: 0.5109 - val_acc: 0.4867 - val_f1macro: 0.4144\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 155s 387ms/step - loss: 0.2976 - acc: 0.7163 - f1macro: 0.6916 - val_loss: 0.5337 - val_acc: 0.4625 - val_f1macro: 0.4431\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 160s 400ms/step - loss: 0.2504 - acc: 0.7611 - f1macro: 0.7459 - val_loss: 0.5344 - val_acc: 0.4586 - val_f1macro: 0.4374\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 165s 412ms/step - loss: 0.2297 - acc: 0.7795 - f1macro: 0.7659 - val_loss: 0.5038 - val_acc: 0.5000 - val_f1macro: 0.4772\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 168s 421ms/step - loss: 0.2194 - acc: 0.7887 - f1macro: 0.7766 - val_loss: 0.5186 - val_acc: 0.4867 - val_f1macro: 0.4702\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 172s 432ms/step - loss: 0.2083 - acc: 0.7989 - f1macro: 0.7871 - val_loss: 0.4802 - val_acc: 0.5234 - val_f1macro: 0.5040\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 157s 393ms/step - loss: 0.2019 - acc: 0.8039 - f1macro: 0.7925 - val_loss: 0.5070 - val_acc: 0.4977 - val_f1macro: 0.4889\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 154s 386ms/step - loss: 0.1979 - acc: 0.8082 - f1macro: 0.7984 - val_loss: 0.4889 - val_acc: 0.5164 - val_f1macro: 0.4987\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 151s 379ms/step - loss: 0.1966 - acc: 0.8089 - f1macro: 0.7980 - val_loss: 0.4970 - val_acc: 0.5055 - val_f1macro: 0.4900\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 150s 376ms/step - loss: 0.1929 - acc: 0.8110 - f1macro: 0.8012 - val_loss: 0.5179 - val_acc: 0.4781 - val_f1macro: 0.4643\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 153s 382ms/step - loss: 0.1876 - acc: 0.8171 - f1macro: 0.8078 - val_loss: 0.5088 - val_acc: 0.4906 - val_f1macro: 0.4655\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 166s 415ms/step - loss: 0.1867 - acc: 0.8172 - f1macro: 0.8082 - val_loss: 0.5020 - val_acc: 0.5000 - val_f1macro: 0.4769\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 166s 417ms/step - loss: 0.1868 - acc: 0.8174 - f1macro: 0.8082 - val_loss: 0.5235 - val_acc: 0.4742 - val_f1macro: 0.4570\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 151s 378ms/step - loss: 0.1781 - acc: 0.8258 - f1macro: 0.8169 - val_loss: 0.4955 - val_acc: 0.5117 - val_f1macro: 0.4964\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 154s 385ms/step - loss: 0.1801 - acc: 0.8235 - f1macro: 0.8140 - val_loss: 0.5157 - val_acc: 0.4844 - val_f1macro: 0.4735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd3787395b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len    = 60\n",
    "batch_size = 128\n",
    "n_epochs   = 20\n",
    "n_features = 82\n",
    "\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b26641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 60, 1, 8)          664       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 29, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 104)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 104)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,169\n",
      "Trainable params: 1,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a748f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 3ms/step\n",
      "accuracy: 0.5209756097560976\n",
      "MAE: 0.4790243902439024\n",
      "F1: 0.5985282093213409\n"
     ]
    }
   ],
   "source": [
    "def testgen(data, seq_len, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    batch = []\n",
    "    for key, df in data.items():\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # find the start of test sample\n",
    "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
    "        n = (df.index == t).argmax()\n",
    "        # extract sample using a sliding window\n",
    "        for i in range(n+1, len(df)+1):\n",
    "            frame = df.iloc[i-seq_len:i]\n",
    "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
    "    X, y = zip(*batch)\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)\n",
    "\n",
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
    " \n",
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a220be",
   "metadata": {},
   "source": [
    "Model accuracy that is above 50% can be considered plausible. Market trend wouldnt be able to get high prediction because of a lot of outside factors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
